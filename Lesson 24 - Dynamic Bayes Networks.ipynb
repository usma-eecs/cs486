{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 24 - Dynamic Bayes Networks\n",
    "\n",
    "Today we will wrap up Markov Models and introduce the **Bayesian Network**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from aima.text import *\n",
    "from aima.probability import *\n",
    "from aima.utils import open_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Bayes' Nets (DBNs)\n",
    "\n",
    "In a traditional Hidden Markov Model, variables only condition on evidence observed in the current time-step and the hidden variable from the previous time-step. **Dynamic Bayes' Nets** allow multiple hidden variables and sources of evidence. There are edges between variables in one time step to variables in future time step where there is a causal relationship between them. \n",
    "\n",
    "![[DBN]](images/dbn.png)\n",
    "\n",
    "DBNs are practically useful in instances where there are multiple sources of evidence. They are also computationally useful since the distributions for multiple hidden variables are smaller than a joint distributions across all of them. \n",
    "\n",
    "### Viterbi Algorithm\n",
    "\n",
    "An HMM encodes the probability distribution of its possible outputs at any given time. Given the output of an HMM the **Viterbi Algorithm** can produce the sequence that most probably produced it. The algorithm is essentially the **Forward Algorithm** that keeps track of the most likely output at every time step: \n",
    "\n",
    "$$m_t[x_t] = P(e_t\\mid{x_t})\\max_{x_{t-1}}P(x_t\\mid{x_{t-1}})m_{t-1}[x_{t-1}]$$\n",
    "\n",
    "For a more visual idea of what's happening, consider the following HMM from [Wikipedia](https://en.wikipedia.org/wiki/Viterbi_algorithm) in which a doctor sees a patient three days in a row. On the first day the patient is normal; on the second he is cold; on the last day he is dizzy. Here is a diagram that capture the transition and emission models for the HMM:\n",
    "\n",
    "![[Viterbi HMM]](images/viterbi_hmm.png)\n",
    "\n",
    "The Viterbi Algorithm can produce the most probable sequence of events that explains the observations:\n",
    "\n",
    "![[Viterbi]](images/viterbi.gif)\n",
    "\n",
    "Consider a sentence without spaces. How do you find the most likely sequence of words? We can use our Unigram model and Viterbi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence of words is: ['she', 'sell', 's', 'sea', 'she', 'll', 's', 'by', 'these', 'as', 'h', 'o', 'r', 'e', '.']\n",
      "Probability of sequence is: 0.0\n"
     ]
    }
   ],
   "source": [
    "flatland = open_data(\"EN-text/flatland.txt\").read()\n",
    "wordseq = words(flatland)\n",
    "\n",
    "P = UnigramWordModel(wordseq)\n",
    "text = \"itiseasytoreadasentencewithoutspaces\"\n",
    "\n",
    "def viterbi_segment(text, P):\n",
    "    \"\"\"Find the best segmentation of the string of characters, given the\n",
    "    UnigramWordModel P.\"\"\"\n",
    "    # best[i] = best probability for text[0:i]\n",
    "    # words[i] = best word ending at position i\n",
    "    n = len(text)\n",
    "    words = [''] + list(text)\n",
    "    best = [1.0] + [0.0] * n\n",
    "    # Fill in the vectors best words via dynamic programming\n",
    "    for i in range(n+1):\n",
    "        for j in range(0, i):\n",
    "            w = text[j:i]\n",
    "            curr_score = P[w] * best[i - len(w)]\n",
    "            if curr_score >= best[i]:\n",
    "                best[i] = curr_score\n",
    "                words[i] = w\n",
    "    # Now recover the sequence of best words\n",
    "    sequence = []\n",
    "    i = len(words) - 1\n",
    "    while i > 0:\n",
    "        sequence[0:0] = [words[i]]\n",
    "        i = i - len(words[i])\n",
    "    # Return sequence of best words and overall probability\n",
    "    return sequence, best[-1]\n",
    "\n",
    "s, p = viterbi_segment(text,P)\n",
    "print(\"Sequence of words is:\",s)\n",
    "print(\"Probability of sequence is:\",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Nets\n",
    "\n",
    "Bayes' Nets are **graphical models** that describe a joint distribution by describing the local conditional probabilities of random variables. Bayes' Nets are directed acyclic graphs in which nodes are random variables and edges are placed between variables that directly interact. Nodes that are not connected are conditionally independent. A Bayes' Net typical (but not necessarily) describes a noisy causal process. \n",
    "\n",
    "A Bayes' Net encodes the joint distribution across the variables without explicitly computing it. Each node carries a conditional distribution given its parents. For example, in the following network $John\\ Calls\\ {\\perp\\!\\!\\!\\perp}\\ Mary\\ Call \\mid{Alarm}$:\n",
    "\n",
    "<img src=\"images/bayes_net.png\" width=\"300\">\n",
    "\n",
    "You can compute the full join across all variables by multiplying all of the conditionals. The probability of a given full assignment is:\n",
    "\n",
    "$$ P(x_1,x_2,x_3,...x_n)=\\prod_{i=1}^nP(x_i\\mid{parents(X_i})$$\n",
    "\n",
    "Note that edges encode interaction between variables, but the direction does not actually matter. Edges only have direction to prevent cycles. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
