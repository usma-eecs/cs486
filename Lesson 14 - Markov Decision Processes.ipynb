{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 14 - Markov Decision Processes\n",
    "\n",
    "*Expectimax* is a way to search a tree for the best action when outcomes are ucertain. In practice, however, we can rarely  search to the root of an expectimax tree. \n",
    "\n",
    "`Markov Decision Processes` (MDPs) are a way of formulating problems such that we can use an *expectimax* approach to establish a **policy** for selecting optimal actions in states without having to perform a search every time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "from aima.mdp import *\n",
    "from aima.notebook import psource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A *Draw HiLo* Policy\n",
    "\n",
    "Last time used `expectimax` to decide which action to choose for a given draw. Our implementation was limited our us to 5 draws since, in theory, the game tree is infinite. \n",
    "\n",
    "Instead of running `expectimax` every time a new card is drawn, let's see if we can use an MDP to create a **policy** which lists the best action to take any given state. Here's what the transition graph looks like for the 5 draw: \n",
    "\n",
    "<img src='images/hilo.svg'>\n",
    "\n",
    "Let's use that information to build an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {\"bet\": -1, \"win\": 1, \"lose\": -1}\n",
    "actions = {\"win\": [\"draw\"],\"lose\":[\"exit\"],\"bet\":[\"draw\"]}\n",
    "transitions = {\"win\": {\"draw\": []}, \"lose\": {\"exit\": []},\"bet\":{\"draw\":[]}}\n",
    "\n",
    "for card in range(1,14):\n",
    "    rewards[card] = 0\n",
    "    actions[card] = [\"higher\",\"lower\"]\n",
    "    \n",
    "    transitions[\"win\"][\"draw\"].append([1/13,card])\n",
    "    transitions[\"bet\"][\"draw\"].append([1/13,card]) \n",
    "    \n",
    "    transitions[card] = {\n",
    "        \"higher\": [[(13-card)/13,\"win\"], [(card-1)/13,\"lose\"]],\n",
    "        \"lower\":  [[(13-card)/13,\"lose\"], [(card-1)/13,\"win\"]]\n",
    "    }\n",
    "\n",
    "class HiLo(MDP):\n",
    "    def __init__(self):\n",
    "        MDP.__init__(\n",
    "            self,\n",
    "            init=\"start\", \n",
    "            actlist=actions,\n",
    "            terminals=[\"lose\"], \n",
    "            transitions=transitions, \n",
    "            reward=rewards, \n",
    "            states=None, \n",
    "            gamma=1)\n",
    "\n",
    "    def actions(self,state):\n",
    "        return self.actlist[state]\n",
    "    \n",
    "hilo = HiLo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our MDP, and we want to know what policy, which is the optimal action from each state. To do that, we need to know what the **expected utility** is at each state. We can use **value iteration** on our MDP to do that. Value iteration is a method to satisfy the **Bellman equations**, which characterizes the optimal expected value of each state.\n",
    "\n",
    "$$ V_{0}(s) \\leftarrow \\overrightarrow{0} $$\n",
    "\n",
    "$$ V_{k+1}(s) \\leftarrow R(s) + \\gamma \\max_{a \\epsilon A(s)} \\sum_{s'} P(s'\\ |\\ s,a) V_{k}(s') $$\n",
    "\n",
    "We perform value iteration on our MDP using the `value_iteration` function which iterates until the difference between the expected values in two successive iterations are less than $\\epsilon $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(value_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_utilities = value_iteration(hilo)\n",
    "expected_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the expected value of a bet in our version *Draw HiLo* is $0.71$. But if you must play, what set of action - or policy - should you follow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy(hilo,expected_utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounting\n",
    "\n",
    "Consider the following MDP for a grid world. You start in the middle of a three grids and you can move left, move right, or stay where you are. If you move left or right you fall into a pit and die. There is a reward for every step that you stay alive. \n",
    "\n",
    "Let's define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridLocked(MDP):\n",
    "    def __init__(self, gamma=1):\n",
    "        MDP.__init__(\n",
    "            self,\n",
    "            init=(1,0), \n",
    "            actlist={\n",
    "                (0,0): [\"exit\"],\n",
    "                (1,0): [\"stay\",\"left\",\"right\"],\n",
    "                (2,0): [\"exit\"]\n",
    "            },\n",
    "            terminals=[(0,0),(2,0)],\n",
    "            transitions={\n",
    "                (0,0): { \"exit\": [] },\n",
    "                (1,0): {\n",
    "                    \"stay\":  [[1,(1,0)]],\n",
    "                    \"left\":  [[1,(0,0)]],\n",
    "                    \"right\": [[1,(2,0)]]\n",
    "                },\n",
    "                (2,0): { \"exit\": [] },\n",
    "            }, \n",
    "            reward={\n",
    "                (0,0): -100,\n",
    "                (1,0): 1,\n",
    "                (2,0): -100\n",
    "            }, \n",
    "            states=None, \n",
    "            gamma=gamma)\n",
    "\n",
    "    def actions(self,state):\n",
    "        return self.actlist[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we use value iteration to compute the expected utility value vector for the MDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridlocked = GridLocked()\n",
    "best_policy(gridlocked,value_iteration(gridlocked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no way to exit! We can either stay still and rack up points or move and die. This is the same problem with `expectimax`, if there is a never-ending path, the algorithm will continue searching it. So what do we do? We discount! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridlocked = GridLocked(gamma=0.9)\n",
    "best_policy(gridlocked,value_iteration(gridlocked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only continue to iterate so long as each iteration is bringing a sufficient change in value. Since discounting makes future rewards less valuable, a discount of less than 1 guarentees that we will stop iterating eventually. Note that this example is deterministic. so it really doesn't make sense to solve it using an MDP. It's just a trivial way to see how discounting can prevent an infinite search. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
