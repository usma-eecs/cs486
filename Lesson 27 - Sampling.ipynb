{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 27 - Sampling\n",
    "\n",
    "Exact inference fails to scale for very large and complex Bayesian Networks. This section covers implementation of randomized sampling algorithms, also called Monte Carlo algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from aima.probability import *\n",
    "from aima.notebook import psource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(BayesNode.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we consider the different algorithms in this section let us look at the **`BayesNode.sample`** method. It samples from the distribution for this variable conditioned on event's values for parent_variables. That is, return True/False at random according to with the conditional probability given the parents. The **`probability`** function is a simple helper from **`utils`** module which returns True with the probability passed to it.\n",
    "\n",
    "### Prior Sampling\n",
    "\n",
    "The idea of Prior Sampling is to sample from the Bayesian Network in a topological order. We start at the top of the network and sample as per **P(X<sub>i</sub> | parents(X<sub>i</sub>)** i.e. the probability distribution from which the value is sampled is conditioned on the values already assigned to the variable's parents. This can be thought of as a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(prior_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **`prior_sample`** implements the algorithm described in **Figure 14.13** of the book. Nodes are sampled in the topological order. The old value of the event is passed as evidence for parent values. We will use the Bayesian Network in **Figure 14.12** to try out the **`prior_sample`**\n",
    "\n",
    "<center><img src=\"aima/images/sprinklernet.jpg\" height=\"500\" width=\"500\"></center>\n",
    "\n",
    "Traversing the graph in topological order is important.\n",
    "There are two possible topological orderings for this particular directed acyclic graph.\n",
    "\n",
    "1. `Cloudy -> Sprinkler -> Rain -> Wet Grass`\n",
    "2. `Cloudy -> Rain -> Sprinkler -> Wet Grass`\n",
    "\n",
    "We can follow any of the two orderings to sample from the network.\n",
    "Any ordering other than these two, however, cannot be used.\n",
    "\n",
    "One way to think about this is that `Cloudy` can be seen as a precondition of both `Rain` and `Sprinkler` and just like we have seen in planning, preconditions need to be satisfied before a certain action can be executed.\n",
    "\n",
    "We store the samples on the observations. Let us find **P(Rain=True)** by taking 1000 random samples from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "all_observations = [prior_sample(sprinkler) for x in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter to get the observations where Rain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rain_true = [observation for observation in all_observations if observation['Rain'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can find $P(Rain=True)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rain_true) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling this another time might give different results as we have no control over the distribution of the random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "all_observations = [prior_sample(sprinkler) for x in range(N)]\n",
    "rain_true = [observation for observation in all_observations if observation['Rain'] == True]\n",
    "len(rain_true) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a conditional distribution. We can use a two-step filtering process. We first separate out the variables that are consistent with the evidence. Then for each value of query variable, we can find probabilities. For example to find **P(Cloudy=True | Rain=True)**. We have already filtered out the values consistent with our evidence in **rain_true**. Now we apply a second filtering step on **rain_true** to find **P(Rain=True and Cloudy=True)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_and_cloudy = [observation for observation in rain_true if observation['Cloudy'] == True]\n",
    "len(rain_and_cloudy) / len(rain_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection Sampling\n",
    "\n",
    "Rejection Sampling is based on an idea similar to what we did just now. First, it generates samples from the prior distribution specified by the network. Then, it rejects all those that do not match the evidence. Rejection sampling is advantageous only when we know the query beforehand. While prior sampling generally works for any query, it might fail in some scenarios.\n",
    "\n",
    "Let's say we have a generic Bayesian network and we have evidence `e`, and we want to know how many times a state `A` is true, given evidence `e` is true. Normally, prior sampling can answer this question, but let's assume that the probability of evidence `e` being true in our actual probability distribution is very small. In this situation, it might be possible that sampling never encounters a data-point where `e` is true. If our sampled data has no instance of `e` being true, `P(e) = 0`, and therefore `P(A | e) / P(e) = 0/0`, which is undefined. We cannot find the required value using this sample. \n",
    "\n",
    "We can definitely increase the number of sample points, but we can never guarantee that we will encounter the case where `e` is non-zero (assuming our actual probability distribution has at least one case where `e` is true).\n",
    "To guarantee this, we would have to consider every single data point, which means we lose the speed advantage that approximation provides us and we essentially have to calculate the exact inference model of the Bayesian network. Rejection sampling will be useful in this situation, as we already know the query.\n",
    "\n",
    "While sampling from the network, we will reject any sample which is inconsistent with the evidence variables of the given query (in this example, the only evidence variable is `e`).\n",
    "We will only consider samples that do not violate **any** of the evidence variables.\n",
    "In this way, we will have enough data with the required evidence to infer queries involving a subset of that evidence.\n",
    "\n",
    "The function **rejection_sampling** implements the algorithm described by **Figure 14.14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(rejection_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function keeps counts of each of the possible values of the Query variable and increases the count when we see an observation consistent with the evidence. It takes the query variable, evidence, the Bayes net and the number of prior samples to generate.\n",
    "\n",
    "**`consistent_with`** is used to check consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(consistent_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer **P(Cloudy=True | Rain=True)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_sampling('Cloudy', {'Rain': True}, sprinkler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Weighting\n",
    "\n",
    "Rejection sampling takes a long time to run when the probability of finding consistent evidence is low. It is also slow for larger networks and more evidence variables.\n",
    "Rejection sampling tends to reject a lot of samples if our evidence consists of a large number of variables. Likelihood Weighting solves this by fixing the evidence (i.e. not sampling it) and then using weights to make sure that our overall sampling is still consistent.\n",
    "\n",
    "The pseudocode in **Figure 14.15** is implemented as **`likelihood_weighting`** and **`weighted_sample`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(weighted_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**`weighted_sample`** samples an event from Bayesian Network that's consistent with the evidence **e** and returns the event and its weight, the likelihood that the event accords to the evidence. It takes in two parameters **bn** the Bayesian Network and **e** the evidence.\n",
    "\n",
    "The weight is obtained by multiplying $P(x_i\\mid{parents(x_i)})$ for each node in evidence. We set the values of **event = evidence** at the start of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sample(sprinkler, {'Rain': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(likelihood_weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`likelihood_weighting`** implements the algorithm to solve our inference problem. The code is similar to **`rejection_sampling`** but instead of adding one for each sample we add the weight obtained from **`weighted_sampling`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_weighting('Cloudy', {'Rain': True}, sprinkler, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "\n",
    "In likelihood sampling, it is possible to obtain low weights in cases where the evidence variables reside at the bottom of the Bayesian Network. This can happen because influence only propagates downwards in likelihood sampling.\n",
    "\n",
    "Gibbs Sampling solves this. The implementation of **Figure 14.16** is provided in the function **gibbs_ask** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(gibbs_ask)\n",
    "psource(markov_blanket_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`gibbs_ask`** we initialize the non-evidence variables to random values. And then select non-evidence variables and sample it from **P(Variable | value in the current state of all remaining vars) ** repeatedly sample. In practice, we speed this up by using **`markov_blanket_sample`** instead. This works because terms not involving the variable get canceled in the calculation. The arguments for **`gibbs_ask`** are similar to **`likelihood_weighting`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gibbs_ask('Cloudy', {'Rain': True}, sprinkler, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime analysis\n",
    "Let's take a look at how much time each algorithm takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "all_observations = [prior_sample(sprinkler) for x in range(1000)]\n",
    "rain_true = [observation for observation in all_observations if observation['Rain'] == True]\n",
    "len([observation for observation in rain_true if observation['Cloudy'] == True]) / len(rain_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "rejection_sampling('Cloudy', {'Rain': True}, sprinkler, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "likelihood_weighting('Cloudy', {'Rain': True}, sprinkler, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "gibbs_ask('Cloudy', {'Rain': True}, sprinkler, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all algorithms have a very similar runtime. However, rejection sampling would be a lot faster and more accurate when the probability of finding data-points consistent with the required evidence is small. Likelihood weighting is the fastest out of all as it doesn't involve rejecting samples, but also has a quite high variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
